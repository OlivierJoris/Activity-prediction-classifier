-- 1st attempt --
KNN with K=1 (knn_basic_1.py). Yield a Kaggle score of 0.52.

-- 2nd attempt --
KNN with K=25 (knn_basic_25.py). Yield a Kaggle score of 0.54.
Motivation: Theoretical - increasing the number of neighbors will the accuracy because we will not be dependent on a single value.

-- 3d attempt --
KNN with K=55 (knn_basic_55.py). Yield a Kaggle score of 0.53142.
Motivation: Yielded highest accuracy on LS with 10-fold cross validation and using negative log loss function as a scoring function.
There was an error in the implementation. So, this is the result of a mistake.

-- 4th attempt --
KNN with K=49 (knn_basic_49.py). Yield a Kaggle score of 0.52571.
Motivation: Yielded highest accuracy on LS with 10-fold cross validation and using negative log loss function as a scoring function.

-- 5th attempt --
Classifier with one KNN per feature. This was motivated by the fact that the units of the different sensors are different.
This yield a Kaggle score of 0.52 (or the same as the toy script).

-- 6th attempt --
Based on 5th, by replacing the lines full of -99999.99 by lines of 0.0.
This yields a Kaggle score of 0.53142 (or the same as 3d attempt).
