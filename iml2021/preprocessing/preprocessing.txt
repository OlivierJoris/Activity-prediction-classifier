1. The dataset is well balanced (e.g. all outputs classes (y_train) have 
exactly a size of 250)

2. The subject_id distribution used to build the dataset is not really uniform used 
to build the dataset : we need to perform feature selection. There are 512 * 3500 
* 31 measures from the sensors, this will also improve the computation time to train our models:
https://scikit-learn.org/stable/modules/feature_selection.html

I think that Variance threshold (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) is really suited to our needs because
we want that similar subject_id measures have a less impact when training to not overfit
on a subject (see preprocessing.py script).

