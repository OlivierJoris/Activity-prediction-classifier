1. The dataset is not really imbalanced (e.g. all outputs classes (y_train) have 
exactly a size of 250). But we can see that there are subject that have been 
more frequently used than others to perform the measurements (preprocssing.py
script) : subject_id are not considered when loading data maybe we should regroup
features per subject_id (internal factors like one subject gets cold faster than 
another).

2. There are 512 * 3500 * 31 measures from the sensors, features selection will 
improve the computation time to train our models:
https://scikit-learn.org/stable/modules/feature_selection.html :

Variance threshold (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) is good to delete features with 
zero variance (irrelevant).

3. There are missing values (-999999.99) that we need to delete because they do 
not help us when training our model actually Variance threshold will delete them.
