\documentclass[a4paper, 11pt, oneside]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{array}
\usepackage{shortvrb}
\usepackage{listings}
\usepackage[fleqn]{amsmath}
\usepackage{amsfonts}
\usepackage{fullpage}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{alltt}
\usepackage{indentfirst}
\usepackage{eurosym}
\usepackage{titlesec, blindtext, color}
\usepackage[table,xcdraw,dvipsnames]{xcolor}
\usepackage[unicode]{hyperref}
\usepackage{url}
\usepackage{float}
\usepackage{subcaption}
\usepackage[skip=1ex]{caption}
\usepackage{dsfont}

\definecolor{brightpink}{rgb}{1.0, 0.0, 0.5}

\usepackage{titling}
\renewcommand\maketitlehooka{\null\mbox{}\vfill}
\renewcommand\maketitlehookd{\vfill\null}

\newcommand{\ClassName}{ELEN-0062: Introduction to Machine Learning}
\newcommand{\ProjectName}{Project 3 - Human Activity Prediction}
\newcommand{\AcademicYear}{2021 - 2022}

%%%% First page settings %%%%

\title{\ClassName\\\vspace*{0.8cm}\ProjectName\vspace{1cm}}
\author{Maxime Goffart \\180521 \and Olivier Joris\\182113}
\date{\vspace{1cm}Academic year \AcademicYear}

\begin{document}

%%% First page %%%
\begin{titlingpage}
{\let\newpage\relax\maketitle}
\end{titlingpage}

\thispagestyle{empty}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Table of contents %%%
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\paragraph{}In this report, we will present and justify the different techniques we used for the third project of the course. The goal is to provide the reasoning behind our choices and to present the results obtained on Kaggle.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data pre-processing}\label{sec:data-pre-process}
\paragraph{}We started by analyzing the data in order to pre-process them. Indeed, pre-processing is important
to have good results when doing classification. This section includes the observations we made about the dataset
and different approaches we tried in order to solve the problems linked to the dataset.

\subsection{Filling missing values}
\paragraph{}The learning set contained a certain number of missing values represented by the \texttt{-999999.99} value. Thus, we started by looking at several 
methods proposed by the scikit-learn library\footnote{\url{https://scikit-learn.org/stable/modules/impute.html}} to replace these missing values. We tried several of these methods and finally decided to use the \texttt{KNNImputer} which completes missing values using k-nearest neighbors. Each missing value is imputed using the mean value from the nearest neighbors found in the training set. 

\subsection{Observations about the samples} \label{sec:obs}

\begin{figure}[H]
\center
\includegraphics[scale = 0.5]{preprocessing/subjects_repartition.png}
\caption{Repartition of the different subject ID's in the learning set.}
\label{subject_repartition}
\end{figure}

\paragraph{}We observed that the learning set seems to be balanced. The activity ID's are well distributed. They are each composed of 250 instances of the subjects in the training set. It is good because if this was imbalanced we could have a model which always predict the value of the majority class. This model would have a nice score on our learning set but when trying it on the testing set, it would lead to bad results. 

\paragraph{}However, when looking at the subject ID's of the learning set, we observed that some subjects have been less used to perform measurements. This can be observed on the figure \ref{subject_repartition}.
This can lead to overfit because all subjects have not the same behavior when perfoming an activity and the sensors measurements are correlated to one subject. We tried to make the sample relative to sensors and did feature selection on them to handle this problem. It was then difficult to predict the activity of the input subjects because the samples were not anymore linked to one subject. We thus, later in the project, realized that when doing cross-validation to measure the accuracy of our models, we had to split our training and testing sets according to the subject ID's in order to see if it can predict the activity of unseen subjects.
We also added these subject ID's as feature.


\subsection{Feature selection}
\paragraph{}The fact that the dataset was really huge (3500 * 31 * 512 values) and some values irrelevant, motivated us to perform features selection. We decided to perform feature selection using \texttt{SelectFromModel}\footnote{\url{https://scikit-learn.org/stable/modules/feature_selection.html}}. It is performing ranking based on one estimator and then remove the less ranked features recursively. We chose the \texttt{ExtraTreeClassifier}\footnote{\url{https://www.geeksforgeeks.org/ml-extra-tree-classifier-for-feature-selection/}} as estimator for this method. It is similar to a random forest   classifier and only differ in the manner it is constructed which lead to the creation of multiple de-correlated decision trees. This seems to be a good model to rank the features because we have seen in the theoretical course that decision trees are nice to discriminate features but are too much related to the dataset. This is why the way the Extremely Randomized Trees are constructed reduce the variance of the dataset and reduce the chance of overfitting.

\paragraph{}The implementation of this part can be found in the \texttt{feature\_selection.py} python script. 

\subsection{Feature extraction} \label{sec:feature_ex}
\paragraph{}Because feature selection did not improve the scores of our model, we decided to extract new features from the ones of the dataset including the subject ID's which were not used in the initial dataset. First, we performed some statistics on the time series (mean, median, standard deviation, etc). 
We had the idea to perform these extractions because it could reduce the number of features and keep the meaning of the data. A null variance implies that the activity is a static activity, the number of negative values implies that it is an activity in a cold environment, etc.
We thought that a good model could understand the meaning of these statistics and it improves our score. 

\paragraph{}Thereafter, because we did not had signal representation knowledge, we investigate\footnote{\url{https://towardsdatascience.com/feature-engineering-on-time-series-data-transforming-signal-data-of-a-smartphone-accelerometer-for-72cbe34b8a60}} about the features we could add to feed our model and found that these statistics could also be expressed in a frequency domain instead of time domain. This is easy to do thanks to the discrete Fourier transforms which are integrated in the numpy library.
We also extracted some purely physics features such as the energy of one signal, the average resultant acceleration, and the signal magnitude area. We also tried some libraries\footnote{Like \texttt{sktime} and \texttt{tsfresh}.} which have this feature extraction as purpose but it was very slow and we thus quickly forgave them.

\paragraph{}Our different attempts of implementation can be found in the \texttt{feature\_extraction\_v*.py} python scripts where * refer to the version of this feature extraction.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Performance measure}
We initially performed cross-validation inside our training set splitting it blindly. This lead us to high scores but when posting our predictions on Kaggle, the score inordinately decreased. Some results we present in the section \ref{sec:models} are based on this technique because we only, late in the project, realized that another method could be used to have more realistic scores.
As introduced in the section \ref{sec:obs}, this method consists in doing cross-validation but taking into account the subject ID's. We decided, at each iteration of our k-cross-validation, to split our training set randomly such the split training set is composed of three of the subject ID's samples and the split testing set of the two remaining. This method is better to approximate predictions on the real testing set because this set is composed of never seen subjects.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Studied models} \label{sec:models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{K-nearest neighbors}
\paragraph{}First, we decided to use the K-nearest neighbors method because it was the one provided by the pedagogical team with the assignment. Yet, the reasons that made us trying multiple versions of this technique are the facts that it is easy to interpret and to use.

\subsubsection{1-NN} \label{subsubsec:1NN}
\paragraph{}Our first submission was the one provided with assignment\footnote{File \texttt{example\_submission.csv}.} that was generated with the 1-nearest neighbor model\footnote{Files \texttt{toy\_script.py} or \texttt{knn\_basic\_1.py}.}.\\
This submission yields a score of 0.52 on Kaggle.

\subsubsection{25-NN} \label{subsubsec:25NN}
\paragraph{}Based on \ref{subsubsec:1NN}, we decided to increase the value of K of the K-nearest neighbors method to 25. The motivation behind the choice of this value for K was theoretical. Since the dataset is huge and can contain errors in the measured values, we though that increasing the value of K would increase the precision of the model because each prediction will not depend on a single neighbor that could have been misclassified.\\
This technique is implemented in the file \texttt{knn\_basic\_25.py}. This submission yields a score of 0.54 on Kaggle.

\subsubsection{55-NN and 49-NN} \label{subsubsec:55-49-NN}
\paragraph{}We decided to keep using the K-NN method to study how well it could perform on the assignment. We decided to study the accuracy of the KNN technique by using a 10-fold cross validation technique with the negative log loss function as a scoring measure. We obtained the following graph of accuracies depending on the value of K:
\begin{figure}[H]
\center
\includegraphics[scale=0.3]{knn/log_loss.png}
\caption{10-CV accuracy of KNN depending on value of K}
\end{figure}
We could see that there is a peak. By using a divide and conquer technique, we found that the peak was obtained for values of K around 50. Thus, we decided to study the accuracy in a small range of K values around $K=50$. We obtained the following graph:
\begin{figure}[H]
\center
\includegraphics[scale=0.4]{knn/log_loss_focused.png}
\caption{10-CV accuracy of KNN depending on value of K}
\label{fig:knn_log_loss_focused}
\end{figure}
\paragraph{}Based on the divide and conquer approach performed, we decided to use 55 neighbors. The choice of this value was motivated by the fact that we observed a peak around this value.\\
This submission yields a score of 0.53142 on Kaggle and the implementation of it is inside the file \texttt{knn\_basic\_55.py}.
\paragraph{}Then, based on the graph of figure \ref{fig:knn_log_loss_focused}, we decided to use 49 neighbors. The choice of this value was motivated by the fact that it yields the highest score on the learning set by using a 10-fold cross validation strategy with the negative log loss function as a scoring measure.\\
This submission yields a score of 0.52571 on Kaggle which is less than the one for 55-NN. Yet, we expected a higher score based on the graph of figure \ref{fig:knn_log_loss_focused}. The implementation is available inside the file \texttt{knn\_basic\_49.py}.
\paragraph{}Based on the two previous results, we came to the conclusion that using "vanilla" K-nearest neighbors was not sufficient. Thus, we decided to find new techniques explained in the following sections.

\subsubsection{Multiple K-nearest neighbors models} \label{subsubsec:knnMultiple}
\paragraph{}We took a look back at the slides provided with the assignment and we focused on the features (sensors) of the datasets. We noticed that the features are not in the same units. Thus, we thought about using multiple 1-nearest neighbor models with one 1-NN model per feature. This idea was motivated by the fact that we though we would obtained a better accuracy by using one 1-NN per feature because measurements in different units would not be mixed.\\
We implemented this approach in the file \texttt{knn\_splitted\_1.py}. This yields a score of 0.52 on Kaggle which is the same as the toy script provided with the assignment.

\subsubsection{Mutiple K-nearest neighbors models and samples modification} \label{subsubsec:knnMultipleModif}
\paragraph{}After the disappointing result obtained in \ref{subsubsec:knnMultiple}, we decided to take a better a look at the datasets. We noticed that some samples were vectors full of -999999.99. We though that these measurements would badly influenced the models. Thus, we decided to replace each sample that was a vector full of -999999.99 by a vector full of 0.\\
We implemented this approach in the file \texttt{knn\_splitted\_1\_filtered.py} and the score obtained on Kaggle was 0.53142.

\subsubsection{Conclusion on K-nearest neighbors}
\paragraph{}After all the different tries perform using K-nearest neighbors models, we came to the conclusion that K-NN was not sufficient for this project.\\
The codes for the different plots are available inside the Jupyter notebooks \texttt{knn\_basic.ipynb} and \texttt{knn\_splitted.ipynb}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Decision trees}\label{subsec:dt}
\paragraph{}Secondly, we decided to use the decision tree method because its interpretability is very easy and the effect of the parameters is easy to understand.

\subsubsection{Decision tree with min\_sample\_split=2} \label{subsubsec:dt2}
\paragraph{}First, we try a very simple decision tree with the parameter \texttt{min\_sample\_split} set to 2. We decided to choose this value for that parameter because we wanted to have a first result for the decision tree on which we could based our further study of the approach.\\
This model has been implemented in the file \texttt{dt\_basic\_2.py}. It has an accuracy of 0.8074285714285715 measured with a 10-fold cross validation strategy and provided a score of 0.38571 on Kaggle.

\subsubsection{Decision tree with post-pruning} \label{subsubsec:dtPruning}
\paragraph{}We decided to study the accuracy of the decision tree depending on the value of the \texttt{min\_sample\_split} parameter. In order to do, we decided to study the accuracy for values of the parameter ranging from 2 to 15500. We obtained the following graph:
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{dt/dt_basic_accuracies_1.png}
\caption{Accuracy of decision tree depending on value of parameter}
\label{fig:dt_acc_1}
\end{figure}
Based on figure \ref{fig:dt_acc_1}, we can see that the value of the parameter, that results in the highest accuracy, is smaller than 2000. Thus, we decided to study the accuracy of the model for values of the parameter between 2 and 2000. We obtained the following graph:
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{dt/dt_basic_accuracies_2.png}
\caption{Accuracy of decision tree depending on value of parameter}
\end{figure}
We continue this approach and study the accuracy of the model for values of the parameter [2,200], [2,25], and [2,5]. We obtained the following graphs:
\begin{table}[H]
\centering
\begin{tabular}{lll}
\includegraphics[scale=0.35]{dt/dt_basic_accuracies_3.png} & \includegraphics[scale=0.35]{dt/dt_basic_accuracies_4.png} & \includegraphics[scale=0.35]{dt/dt_basic_accuracies_5.png}
\end{tabular}
\end{table}
It turns out that using that using a small value for the parameter \texttt{min\_sample\_split} provides a higher accuracy using a 10-fold cross validation strategy. Yet, we might overfit on unseen data. Thus, we decided to study the overfitting. We based ourselves on the approach presented in the documentation of scikit-learn\footnote{https://scikit-learn.org/stable/auto\_examples/tree/plot\_cost\_complexity\_pruning.html\#}.
\paragraph{}We split the learning sample in 2 using proportions 75\% and 25\% where the 75\% will be our new learning set and the 25\% will be our new test set. Then, we computed the path during the minimal cost-complexity pruning and we got the alphas for the sub-trees. Afterwards, we build a tree with the different values of alphas obtained in the previous step\footnote{We actually build only a fifth because there was too much possible alphas.}. Finally, we computed the score of the LS and the TS on the different tress and plotted it. We obtained the following graphs\footnote{The graph on the right is a zoom on the graph on the left.}:
\begin{table}[H]
\centering
\begin{tabular}{ll}
\includegraphics[scale=0.35]{dt/dt_pruning_study.png} & \includegraphics[scale=0.35]{dt/dt_pruning_study_zoomed.png}
\end{tabular}
\end{table}
Based on the obtained data, we found out that the value of alpha that provides the highest score on the test set was 0.0006772486772486773.\\
So, for our next model, we built a decision tree with this particular value of alpha for pruning. This particular model provided an accuracy of 0.8025714285714285 using a 10-fold cross validation approach and a score of 0.40285 on Kaggle, which is small improvement compared to the score 0.38571 obtained without pruning.\\
The analysis performed for this subsection is available in the Jupyter notebook \texttt{dt.ipnyb} and the implementation of the model in the file \texttt{dt\_pruned.py}.

\subsubsection{Decision tree with filtered data} \label{subsubsec:dtFiltered}
\paragraph{}As explained in section \ref{sec:data-pre-process}, the data provided is not ideal. Thus, we decided to try a decision tree with pre-processed data.\\
This model is implemented in the file \texttt{dt\_filtered.py}. This particular model provided an accuracy of 0.7928571428571429 using a 10-fold cross validation approach\footnote{Available in the Jupyter notebook \texttt{dt\_filtered.ipynb}.} and a score of 0.43428 on Kaggle. This is an improvement compared to the 0.40285 obtained with pruning.

\subsubsection{Conclusion on decision trees}
\paragraph{}After the study performed on decision trees, we concluded that decision trees were not adequate for this project due to the complexity of the dataset. We decided to move to more complex models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Random forests}
\paragraph{}Next, we decided to use random forests because they should provide better accuracies than decision trees. Yet, their interpretability is still easy.

\subsubsection{Random forest with 40 trees} \label{subsubsec:rf40Trees}
\paragraph{}First, we decided to try a few forests and measuring their accuracies using a 10-fold cross validation strategy. We obtained the following results:
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{\#}} & \multicolumn{1}{c|}{\textbf{Number of trees}} & \multicolumn{1}{c|}{\textbf{min\_sample\_split}} & \multicolumn{1}{c|}{\textbf{10-CV accuracy}} \\ \hline
1 & 10 & 200 & 0.7354285714285714 \\ \hline
2 & 50 & 200 & 0.7702857142857142 \\ \hline
3 & 50 & 100 & 0.8608571428571429 \\ \hline
4 & 100 & 50 & 0.9062857142857143 \\ \hline
\end{tabular}
\caption{10-CV accuracy of different random forests}
\label{tab:rf-acc}
\end{table}
Based on table \ref{tab:rf-acc}, the first conclusion we draw was that increasing the number of trees and diminishing the value of \texttt{min\_sample\_split} increases the accuracy measured with a 10-fold cross validation approach.
\paragraph{}Then, we decided to study the accuracy of the forest based on the number of trees with a fixed value for \texttt{min\_sample\_split} (arbitrarily set to 25). We obtained the following graph:
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{rf/rf_basic_accuracies.png}
\caption{Accuracy of random forests depending on number of trees}
\label{fig:rf-acc-1}
\end{figure}
Based on figure \ref{fig:rf-acc-1}, we can see that, for a number of trees greater or equal to 30, we reach a plateau. Now, we will study the accuracy of random forests depending on the value of \texttt{min\_sample\_split} for a number of trees fixed to 40. We obtained the following graph:
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{rf/rf_40_accuracies.png}
\caption{Accuracy of random forests depending on \texttt{min\_sample\_split}}
\label{fig:rf-acc-2}
\end{figure}
As we can see on figure \ref{fig:rf-acc-2}, the smaller the value of \texttt{min\_sample\_split}, the higher the accuracy using a 10-fold cross validation measure.
\paragraph{}Based on a what we have observed, we decided to try a random forest composed of 40 trees with \texttt{min\_sample\_split} set to 25.\\
This model is implemented in the file \texttt{rf\_40\_25.py} and the analysis performed is available in the Jupyter notebook \texttt{random\_forest.ipynb}.\\
This model provided an accuracy of 0.9288571428571428 using a 10-fold cross validation strategy and a score of 0.52 on Kaggle. This score on Kaggle is the same as the one obtained for 1-NN. Yet, this is an improvement compared to the scores obtained for the decisions tree in the part \ref{subsec:dt} of this report. This is what was expected because random forests are ensemble methods that are using multiple trees.

\subsubsection{Random forest with feature selection} \label{subsubsec:rfFiltered}
\paragraph{}As explained in section \ref{sec:data-pre-process}, the data provided is not ideal. Thus, we decided to try a random forest with pre-processed data. We kept the same values for the number of trees and \texttt{min\_sample\_split}.\\
This model is implemented in the file \texttt{rf\_filtered.py}. This particular model provides an accuracy of 0.9074285714285713 using a 10-fold cross validation approach\footnote{Available in the Jupyter notebook \texttt{random\_forest\_filtered.ipynb}.} and a score of 0.48285 on Kaggle. This is less than what we obtained for a random forest without filtering the data. This is certainly linked to the fact that our first approach for feature selection was not ideal as explained in section \ref{sec:data-pre-process}.

\subsubsection{Random forest with feature extraction} \label{subsubsec:rf_feature_extraction}
\paragraph{}The, we used the feature extraction of the section \ref{sec:feature_ex} and obtained these results locally using the cross-validation and splitting according to the subject ID's: 

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{rf/random_forest_feature_extraction.png}
\caption{Accuracy of random forests depending on the number of trees.}
\label{fig:rf-feature-extraction}
\end{figure}

\paragraph{}The way the features were generated is well appropriate to trees. They can, for example, split according to the feature which represents the variance of the timesteps. The tree can divide the ones having a null variance from the others which represents splitting static activities from dynamic ones. It is why we chose this model as final model. We have less chance to overfit because we understand our model unlike models which can become too complex to understand. It also performs the best score when doing cross-validation splitting according to the subject ID's which motivated this choice.

\paragraph{}The implementation of this model is available in the \texttt{26\_random\_forest\_feature\_extraction.py} python script.

\subsubsection{Conclusion on random forests}
\paragraph{}Random forests which is an ensemble method using "bagged" trees fill the trend of the trees to overfit and is thus well adapted to our feature extraction. As observable in the table of section \ref{sec:summary}, random forest is the model that provides the highest accuracy locally and on Kaggle. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Logistic regression} \label{subsec:lr}

\paragraph{}We tried to use the logistic regression model on our extracted features. Indeed, logistic regression being a linear classifier, it relies upon the quality of the features in order to perform good predictions.
When trying this model, we observed a warning concerning the convergence saying that it was better to standardize the data. Thus, we did it but the logistic regression did not result in good predictions. It was really often predicting the running activity. 
Thus, we concluded that our features were too complex for this model. 

\paragraph{}The implementation of this part can be found in the \texttt{logistic\_regression\_feature\_extraction.py} python script.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Multilayer perceptron} \label{subsec:mlp}
\paragraph{}We tried to use single and multi layers perceptron classifiers as models, both with feature selection and feature extraction because the initial dataset was too huge to train this model. The main parameters of this model
are the number of neurons and the number of layers. Thus, we studied the model according to these two parameters.

\subsubsection{Feature selection}

\paragraph{}The evolution of the model with feature selection according to the number of neurons and layers can be observed on the figures \ref{fig:mlp-feature-selection-neurons} and \ref{fig:mlp-feature-selection-layers}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{mlp/MLP_feature_selection_neurons.png}
\caption{Accuracy of MLP depending on the number of neurons (single layer).}
\label{fig:mlp-feature-selection-neurons}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{mlp/MLP_feature_selection_layers.png}
\caption{Accuracy of MLP depending on the number of layers with 50 neurons per layer.}
\label{fig:mlp-feature-selection-layers}
\end{figure}

\paragraph{}The implementation of this part can be found in the \texttt{MLP\_feature\_selection.py} python script.

\subsubsection{Feature extraction} \label{subsubsec:mlp-feature-extraction}

\paragraph{}The evolution of this model with feature extraction according to the number of neurons and layers can be observed on the figures \ref{fig:mlp-feature-extraction-neurons} and \ref{fig:mlp-feature-extraction-layers}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{mlp/MLP_feature_extraction_neurons.png}
\caption{Accuracy of MLP depending on the number of neurons (single layer).}
\label{fig:mlp-feature-extraction-neurons}
\end{figure}
    
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{mlp/MLP_feature_extraction_layers.png}
\caption{Accuracy of MLP depending on the number of layers with 50 neurons per layer.}
\label{fig:mlp-feature-extraction-layers}
\end{figure}

\paragraph{}The implementation of this part can be found in the \texttt{MLP\_feature\_extraction.py} python script.

\subsubsection{Conclusion on Multilayer perceptron}

\paragraph{}We did not investigate much on multilayer perceptron classifier because we observed that this model was hard to tune. The graphics we presented in this section do not give real information about what the optimal parameters are.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Support vector machines}
\paragraph{}For completeness, we wanted to try support vector machines.

\subsubsection{Support vector classification} \label{subsubsec:svc}
\paragraph{}This approach is based on the model SVC of scikit-learn\footnote{sklearn.svm.SVC.}. We decided to study the accuracy of this model by splitting the learning set based on the ID's of the 5 subjects and by using feature selection.\\
For our first study, we decided to consider a wide range for the value of the regularization parameter\footnote{Denoted C in scikit-learn.}. We choose the range [1.0, 1000.0] has a starting point and obtained the following graph of accuracies depending on the value of the parameter:
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{svm/svm_svc_accuracies_1.png}
\caption{Accuracy of SVC depending on regularization parameter}
\end{figure}
As we can see on the previous plot, it seems there is not an obvious relation between the regularization and the accuracy of the model. Thus, we decided to study the accuracy of the model for values of the regularization in the range [1.0, 100.0]. We obtained the following graph:
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{svm/svm_svc_accuracies_2.png}
\caption{Accuracy of SVC depending on regularization parameter}
\end{figure}
Once again, it seems there is no obvious relation between the regularization and the accuracy. Yet, the values for the previous graphs were obtained as averages on 10 tries. We decided to increase the number of tries to 20. We obtained the following graph:
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{svm/svm_svc_accuracies_4.png}
\caption{Accuracy of SVC depending on regularization parameter}
\end{figure}
The peak observed on the graph is for a value of the regularization parameter of 60. Thus, we decided to use a SVM (specifically \texttt{sklearn.svm.SVC}) with a value of 60 as the regularization parameter. This models has an accuracy 0.5956468368018641 locally obtained by splitting the learning set based on the ID's of the 5 subjects and by using feature selection\footnote{The accuracies are averages over 20 tries.}. This model gives a score of 0.49428 on Kaggle.\\
The study performed for this model is available in the Jupyter notebook \texttt{svc-feature-extract\\ion.ipynb} and the implementation of the model in the file \texttt{svc-reg-60.py}.

\subsubsection{Support vector classification - $\nu$-SVC} \label{subsubsec:nusvc}
\paragraph{}This approach is based on the model $\nu$-SVC of scikit-learn\footnote{sklearn.svm.NuSVC.}. As always, we decided to study the accuracy of the model depending on the value of the parameter $\nu$. To study the accuracy, we split the learning set based on the ID's of the 5 subjects and by calculating average accuracies over multiples splits. We used feature selection to reduce the size of the learning set.\\
We study the accuracy of the model for values of $\nu$ in [0.1, 0.6]. We obtained the following graph:
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{nusvm/svm_nu_svc_accuracies_1.png}
\caption{Accuracy of $\nu$-SVC depending on value of $\nu$}
\end{figure}
The peak observed on the graph is for a value of $\nu$ of 0.5. Thus, we decided to use a $\nu$-SVC with $\nu = 0.5$. This model has an accuracy 0.5723277462764307 locally measured as explained before and a score on Kaggle of 0.56571.\\
The code for the generation of the graph is available in the Jupyter notebook \texttt{nusvc-feature-\\extraction.ipynb} and the implementation of the model in the file \texttt{nusvc.py}.

\subsubsection{Conclusion on SVM}
\paragraph{}Support vector machines provide accuracies that are nowhere near the ones provided by random forests. Thus, we decided to stop our exploration of support vector machines and focus on random forests and improving our feature extraction technique.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary table} \label{sec:summary}
\paragraph{}The following table summaries the techniques we have tried and the scores we have obtained on Kaggle.

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Approach}}                     & \multicolumn{1}{c|}{\textbf{Report section}} & \multicolumn{1}{c|}{\textbf{Kaggle score}} \\ \hline
1-NN                                                        & \ref{subsubsec:1NN}                          & 0.52                                       \\ \hline
25-NN                                                       & \ref{subsubsec:25NN}                         & 0.54                                       \\ \hline
55-NN                                                       & \ref{subsubsec:55-49-NN}                     & 0.53142                                    \\ \hline
49-NN                                                       & \ref{subsubsec:55-49-NN}                     & 0.52571                                    \\ \hline
Multiple 1-NN                                               & \ref{subsubsec:knnMultiple}                  & 0.52                                       \\ \hline
Multiple 1-NN with filtered data                            & \ref{subsubsec:knnMultipleModif}             & 0.53142                                    \\ \hline
Decision tree - min\_sample\_split=2                        & \ref{subsubsec:dt2}                          & 0.38571                                    \\ \hline
Decision tree with pruning                                  & \ref{subsubsec:dtPruning}                    & 0.40285                                    \\ \hline
Decision tree with filtered data                            & \ref{subsubsec:dtFiltered}                   & 0.43428                                    \\ \hline
Random forest - 40 trees - min\_sample\_split=25            & \ref{subsubsec:rf40Trees}                    & 0.52                                       \\ \hline
Random forest - filtered data                               & \ref{subsubsec:rfFiltered}                   & 0.48285                                    \\ \hline
MLP - 1 inner layer - 500 neurons                           & \ref{subsec:mlp}                               & 0.48285                                    \\ \hline
MLP - 1 inner layer - 100 neurons                           & \ref{subsec:mlp}                                 & 0.43428                                    \\ \hline
MLP - 1 inner layer - 100 neurons - feature extraction      & \ref{subsubsec:mlp-feature-extraction}                                  & 0.22                                       \\ \hline
25-NN with feature extraction                               & Not presented (too poor)                              & 0.45714                                    \\ \hline
Random forest - 1000 trees with feature extraction          & \ref{subsubsec:rf_feature_extraction}                                & 0.87428                                    \\ \hline
Logistic regression with feature extraction                 & \ref{subsec:lr}                               & 0.48285                                    \\ \hline
SVM with feature extraction                                 & Not presented (too poor)                              & 0.07142                                    \\ \hline
SGD with feature extraction                                 & Not presented (too poor)                         & 0.07142                                    \\ \hline
SVM with feature extraction                                 & \ref{subsubsec:svc}                          & 0.49428                                    \\ \hline
NuSVC with feature extraction                               & \ref{subsubsec:nusvc}                        & 0.56571                                    \\ \hline
Random forest - 1000 trees with improved feature extraction & \ref{subsubsec:rf_feature_extraction}                                 & 0.91142                                    \\ \hline
Random forest - 400 trees with improved feature extraction & \ref{subsubsec:rf_feature_extraction}                                 & 0.91714                                   \\ \hline
\end{tabular}
}
\caption{Approaches with respective Kaggle scores}
\end{table}

\section{Conclusion}

\paragraph{}This project allowed us to apply a lot of the techniques and models studied in the theoretical course. It helped us to understand certain issues related to a classification problem.
\paragraph{}The model we chose seemed to well perform on the public leaderboard but a bit overfitted it. However, this model is the second best one we had submitted with a difference of 0.004 of accuracy comparing it to our best model. We thus not regret our choice. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}